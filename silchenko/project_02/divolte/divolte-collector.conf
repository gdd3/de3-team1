divolte {
  global {
    server {
      host = 0.0.0.0
      port = 8290
    }
    hdfs {
      client {
        fs.defaultFS = "hdfs://instance-1.europe-west1-b.c.pro-signal-218407.internal:8020/"
      }
      // Enable HDFS sinks.
      enabled = true

      // Use multiple threads to write to HDFS.
      threads = 2
    }
    kafka {
      // If true, flushing to Kafka is enabled.
      enabled = true

      // Number of threads to use for flushing events to Kafka
      threads = 2

      // The maximum queue of mapped events to buffer before
      // starting to drop new ones. Note that when this buffer is full,
      // events are dropped and a warning is logged. No errors are reported
      // to the source of the events. A single buffer is shared between all
      // threads, and its size will be rounded up to the nearest power of 2.
      buffer_size = 1048576

      // All settings in here are used as-is to configure
      // the Kafka producer.
      // See: http://kafka.apache.org/082/documentation.html#newproducerconfigs
      producer = {
        bootstrap.servers = ["instance-1.europe-west1-b.c.pro-signal-218407.internal:6667"]
        client.id = divolte.collector

        acks = 1
        retries = 0
        // compression.type = lz4
        max.in.flight.requests.per.connection = 1
      }
    }
  }
  
  sinks {
    // The name of the sink. (It's referred to by the mapping.)
    hdfs {
      type = hdfs

      // For HDFS sinks we can control how the files are created.
      file_strategy {
        // Create a new file every hour
        roll_every = 1 hour

        // Perform a hsync call on the HDFS files after every 1000 records are written
        // or every 5 seconds, whichever happens first.

        // Performing a hsync call periodically can prevent data loss in the case of
        // some failure scenarios.
        sync_file_after_records = 1000
        sync_file_after_duration = 5 seconds

        // Files that are being written will be created in a working directory.
        // Once a file is closed, Divolte Collector will move the file to the
        // publish directory. The working and publish directories are allowed
        // to be the same, but this is not recommended.
        working_dir = "/divolte/inflight"
        publish_dir = "/divolte/published"
      }

      // Set the replication factor for created files.
      replication = 3
    }
  
  kafka {
      type = kafka

      // This is the name of the topic that data will be produced on
      topic = egor.silchenko
    }
  }
  
  mappings {
    my_mapping = {
      schema_file = "/opt/divolte/conf/CheckoutEvent.avsc"
      mapping_script_file = "/opt/divolte/conf/mapping.groovy"
      sources = [browser]
      sinks = [kafka, hdfs]
    }
  }
  
  sources {
    browser {
      type = browser
    }
  }
}
